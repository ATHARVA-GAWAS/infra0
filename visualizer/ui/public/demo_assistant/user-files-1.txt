{
  "files": [
    {
      "path": "/Users/ashutoshsingh/Desktop/personal/infra/infra-cursor/cli/aws-data-pipeline/Pulumi.yaml",
      "content": "name: aws-data-pipeline\nruntime: nodejs\ndescription: AWS Data Processing Pipeline with API Gateway, Kinesis, Lambda, SQS, and DynamoDB\n\ntemplate:\n  config:\n    aws:region:\n      description: The AWS region to deploy into\n      default: us-east-1 "
    },
    {
      "path": "/Users/ashutoshsingh/Desktop/personal/infra/infra-cursor/cli/aws-data-pipeline/index.ts",
      "content": "import * as pulumi from \"@pulumi/pulumi\";\nimport * as aws from \"@pulumi/aws\";\nimport * as awsx from \"@pulumi/awsx\";\n\n// Get configuration\nconst config = new pulumi.Config();\nconst region = config.get(\"aws:region\") || \"us-east-1\";\n\n// ========== S3 Bucket for Firehose delivery ==========\nconst dataBucket = new aws.s3.Bucket(\"data-lake-bucket\", {\n    bucket: `data-pipeline-lake-${pulumi.getStack()}`,\n    forceDestroy: true,\n    tags: {\n        Name: \"Data Lake Bucket\",\n        Environment: pulumi.getStack(),\n    },\n});\n\nconst dataBucketVersioning = new aws.s3.BucketVersioning(\"data-bucket-versioning\", {\n    bucket: dataBucket.id,\n    versioningConfiguration: {\n        status: \"Enabled\",\n    },\n});\n\nconst dataTable = new aws.dynamodb.Table(\"processed-data-table\", {\n    name: `processed-data-${pulumi.getStack()}`,\n    billingMode: \"PAY_PER_REQUEST\",\n    hashKey: \"id\",\n    attributes: [\n        {\n            name: \"id\",\n            type: \"S\",\n        },\n    ],\n    ttl: {\n        attributeName: \"ttl\",\n        enabled: true,\n    },\n    tags: {\n        Name: \"Processed Data Table\",\n        Environment: pulumi.getStack(),\n    },\n});\n\n// ========== Kinesis Stream ==========\nconst dataStream = new aws.kinesis.Stream(\"data-stream\", {\n    name: `data-stream-${pulumi.getStack()}`,\n    shardCount: 1,\n    retentionPeriod: 24,\n    shardLevelMetrics: [\n        \"IncomingRecords\",\n        \"OutgoingRecords\",\n    ],\n    tags: {\n        Name: \"Data Stream\",\n        Environment: pulumi.getStack(),\n    },\n});\n\n// ========== IAM Roles ==========\n\n// Firehose Role\nconst firehoseRole = new aws.iam.Role(\"firehose-role\", {\n    assumeRolePolicy: JSON.stringify({\n        Version: \"2012-10-17\",\n        Statement: [\n            {\n                Action: \"sts:AssumeRole\",\n                Effect: \"Allow\",\n                Principal: {\n                    Service: \"firehose.amazonaws.com\",\n                },\n            },\n        ],\n    }),\n});\n\nconst firehosePolicy = new aws.iam.RolePolicy(\"firehose-policy\", {\n    role: firehoseRole.id,\n    policy: pulumi.all([dataBucket.arn, dataStream.arn]).apply(([bucketArn, streamArn]: [string, string]) =>\n        JSON.stringify({\n            Version: \"2012-10-17\",\n            Statement: [\n                {\n                    Effect: \"Allow\",\n                    Action: [\n                        \"s3:AbortMultipartUpload\",\n                        \"s3:GetBucketLocation\",\n                        \"s3:GetObject\",\n                        \"s3:ListBucket\",\n                        \"s3:ListBucketMultipartUploads\",\n                        \"s3:PutObject\",\n                    ],\n                    Resource: [bucketArn, `${bucketArn}/*`],\n                },\n                {\n                    Effect: \"Allow\",\n                    Action: [\n                        \"kinesis:DescribeStream\",\n                        \"kinesis:GetShardIterator\",\n                        \"kinesis:GetRecords\",\n                        \"kinesis:ListShards\",\n                    ],\n                    Resource: streamArn,\n                },\n                {\n                    Effect: \"Allow\",\n                    Action: [\n                        \"logs:PutLogEvents\",\n                    ],\n                    Resource: \"arn:aws:logs:*\",\n                },\n            ],\n        })\n    ),\n});\n\n// Lambda Execution Role\nconst lambdaRole = new aws.iam.Role(\"lambda-execution-role\", {\n    assumeRolePolicy: JSON.stringify({\n        Version: \"2012-10-17\",\n        Statement: [\n            {\n                Action: \"sts:AssumeRole\",\n                Effect: \"Allow\",\n                Principal: {\n                    Service: \"lambda.amazonaws.com\",\n                },\n            },\n        ],\n    }),\n});\n\nconst lambdaPolicy = new aws.iam.RolePolicy(\"lambda-policy\", {\n    role: lambdaRole.id,\n    policy: pulumi.all([dataTable.arn, dataStream.arn]).apply(([tableArn, streamArn]: [string, string]) =>\n        JSON.stringify({\n            Version: \"2012-10-17\",\n            Statement: [\n                {\n                    Effect: \"Allow\",\n                    Action: [\n                        \"logs:CreateLogGroup\",\n                        \"logs:CreateLogStream\",\n                        \"logs:PutLogEvents\",\n                    ],\n                    Resource: \"arn:aws:logs:*:*:*\",\n                },\n                {\n                    Effect: \"Allow\",\n                    Action: [\n                        \"dynamodb:BatchGetItem\",\n                        \"dynamodb:BatchWriteItem\",\n                        \"dynamodb:DeleteItem\",\n                        \"dynamodb:GetItem\",\n                        \"dynamodb:PutItem\",\n                        \"dynamodb:Query\",\n                        \"dynamodb:Scan\",\n                        \"dynamodb:UpdateItem\",\n                    ],\n                    Resource: tableArn,\n                },\n                {\n                    Effect: \"Allow\",\n                    Action: [\n                        \"kinesis:DescribeStream\",\n                        \"kinesis:GetShardIterator\",\n                        \"kinesis:GetRecords\",\n                        \"kinesis:ListShards\",\n                        \"kinesis:PutRecord\",\n                        \"kinesis:PutRecords\",\n                    ],\n                    Resource: streamArn,\n                },\n            ],\n        })\n    ),\n});\n\n// ========== Kinesis Firehose ==========\nconst deliveryStream = new aws.kinesis.FirehoseDeliveryStream(\"data-firehose\", {\n    name: `data-firehose-${pulumi.getStack()}`,\n    destination: \"s3\",\n    s3Configuration: {\n        roleArn: firehoseRole.arn,\n        bucketArn: dataBucket.arn,\n        prefix: \"year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/\",\n        errorOutputPrefix: \"errors/\",\n        bufferingSize: 5,\n        bufferingInterval: 300,\n        compressionFormat: \"GZIP\",\n    },\n    tags: {\n        Name: \"Data Firehose\",\n        Environment: pulumi.getStack(),\n    },\n});\n\n// ========== Lambda Functions ==========\n\n// API Lambda\nconst apiLambda = new aws.lambda.Function(\"api-lambda\", {\n    name: `api-lambda-${pulumi.getStack()}`,\n    code: new pulumi.asset.FileArchive(\"./lambda\"),\n    handler: \"api-lambda.handler\",\n    runtime: \"nodejs18.x\",\n    role: lambdaRole.arn,\n    timeout: 30,\n    environment: {\n        variables: {\n            DYNAMODB_TABLE_NAME: dataTable.name,\n        },\n    },\n    tags: {\n        Name: \"API Lambda\",\n        Environment: pulumi.getStack(),\n    },\n});\n\n// Streaming Lambda\nconst streamingLambda = new aws.lambda.Function(\"streaming-lambda\", {\n    name: `streaming-lambda-${pulumi.getStack()}`,\n    code: new pulumi.asset.FileArchive(\"./lambda\"),\n    handler: \"streaming-lambda.handler\",\n    runtime: \"nodejs18.x\",\n    role: lambdaRole.arn,\n    timeout: 300,\n    environment: {\n        variables: {\n            DYNAMODB_TABLE_NAME: dataTable.name,\n        },\n    },\n    tags: {\n        Name: \"Streaming Lambda\",\n        Environment: pulumi.getStack(),\n    },\n});\n\n// Kinesis Ingest Lambda\nconst kinesisIngestLambda = new aws.lambda.Function(\"kinesis-ingest-lambda\", {\n    name: `kinesis-ingest-lambda-${pulumi.getStack()}`,\n    code: new pulumi.asset.FileArchive(\"./lambda\"),\n    handler: \"kinesis-ingest-lambda.handler\",\n    runtime: \"nodejs18.x\",\n    role: lambdaRole.arn,\n    timeout: 30,\n    environment: {\n        variables: {\n            KINESIS_STREAM_NAME: dataStream.name,\n        },\n    },\n    tags: {\n        Name: \"Kinesis Ingest Lambda\",\n        Environment: pulumi.getStack(),\n    },\n});\n\n// ========== Event Source Mappings ==========\n\n// Kinesis to Streaming Lambda\nconst kinesisEventSourceMapping = new aws.lambda.EventSourceMapping(\"kinesis-lambda-mapping\", {\n    eventSourceArn: dataStream.arn,\n    functionName: streamingLambda.arn,\n    startingPosition: \"LATEST\",\n    batchSize: 100,\n    maximumBatchingWindowInSeconds: 5,\n});\n\n// ========== API Gateway ==========\nconst api = new awsx.apigateway.API(\"data-pipeline-api\", {\n    routes: [\n        {\n            path: \"/api/data\",\n            method: \"GET\",\n            eventHandler: apiLambda,\n        },\n        {\n            path: \"/api/data/{id}\",\n            method: \"GET\", \n            eventHandler: apiLambda,\n        },\n        {\n            path: \"/api/ingest/stream\",\n            method: \"POST\",\n            eventHandler: kinesisIngestLambda,\n        },\n    ],\n});\n\n// ========== Outputs ==========\nexport const apiUrl = api.url;\nexport const kinesisStreamName = dataStream.name;\nexport const firehoseDeliveryStreamName = deliveryStream.name;\nexport const s3BucketName = dataBucket.id;\nexport const dynamoTableName = dataTable.name;\nexport const apiLambdaArn = apiLambda.arn;\nexport const streamingLambdaArn = streamingLambda.arn;\nexport const kinesisIngestLambdaArn = kinesisIngestLambda.arn; "
    },
    {
      "path": "/Users/ashutoshsingh/Desktop/personal/infra/infra-cursor/cli/aws-data-pipeline/lambda/api-lambda.js",
      "content": "const AWS = require('aws-sdk');\nconst dynamodb = new AWS.DynamoDB.DocumentClient();\n\nexports.handler = async (event) => {\n    try {\n        const tableName = process.env.DYNAMODB_TABLE_NAME;\n        \n        // Extract query parameters\n        const queryParams = event.queryStringParameters || {};\n        const { id, limit = 10 } = queryParams;\n        \n        let result;\n        \n        if (id) {\n            // Get specific item by ID\n            const params = {\n                TableName: tableName,\n                Key: { id: id }\n            };\n            result = await dynamodb.get(params).promise();\n            \n            return {\n                statusCode: 200,\n                headers: {\n                    'Content-Type': 'application/json',\n                    'Access-Control-Allow-Origin': '*'\n                },\n                body: JSON.stringify({\n                    success: true,\n                    data: result.Item || null\n                })\n            };\n        } else {\n            // Scan for multiple items with limit\n            const params = {\n                TableName: tableName,\n                Limit: parseInt(limit)\n            };\n            result = await dynamodb.scan(params).promise();\n            \n            return {\n                statusCode: 200,\n                headers: {\n                    'Content-Type': 'application/json',\n                    'Access-Control-Allow-Origin': '*'\n                },\n                body: JSON.stringify({\n                    success: true,\n                    data: result.Items,\n                    count: result.Count\n                })\n            };\n        }\n        \n    } catch (error) {\n        console.error('Error:', error);\n        \n        return {\n            statusCode: 500,\n            headers: {\n                'Content-Type': 'application/json',\n                'Access-Control-Allow-Origin': '*'\n            },\n            body: JSON.stringify({\n                success: false,\n                error: 'Internal server error'\n            })\n        };\n    }\n}; "
    },
    {
      "path": "/Users/ashutoshsingh/Desktop/personal/infra/infra-cursor/cli/aws-data-pipeline/lambda/kinesis-ingest-lambda.js",
      "content": "const AWS = require('aws-sdk');\nconst kinesis = new AWS.Kinesis();\n\nexports.handler = async (event) => {\n    console.log('Received Kinesis ingest request:', JSON.stringify(event, null, 2));\n    \n    const streamName = process.env.KINESIS_STREAM_NAME;\n    \n    try {\n        const body = JSON.parse(event.body || \"{}\");\n        \n        const params = {\n            StreamName: streamName,\n            Data: JSON.stringify({\n                ...body,\n                ingestedAt: new Date().toISOString(),\n                requestId: event.requestContext?.requestId || 'unknown'\n            }),\n            PartitionKey: body.partitionKey || Date.now().toString(),\n        };\n        \n        const result = await kinesis.putRecord(params).promise();\n        console.log('Successfully sent to Kinesis:', result);\n        \n        return {\n            statusCode: 200,\n            headers: {\n                \"Content-Type\": \"application/json\",\n                \"Access-Control-Allow-Origin\": \"*\",\n                \"Access-Control-Allow-Methods\": \"POST, OPTIONS\",\n                \"Access-Control-Allow-Headers\": \"Content-Type\"\n            },\n            body: JSON.stringify({\n                success: true,\n                message: \"Data sent to Kinesis stream\",\n                shardId: result.ShardId,\n                sequenceNumber: result.SequenceNumber\n            }),\n        };\n        \n    } catch (error) {\n        console.error('Error sending data to Kinesis:', error);\n        \n        return {\n            statusCode: 500,\n            headers: {\n                \"Content-Type\": \"application/json\",\n                \"Access-Control-Allow-Origin\": \"*\",\n                \"Access-Control-Allow-Methods\": \"POST, OPTIONS\",\n                \"Access-Control-Allow-Headers\": \"Content-Type\"\n            },\n            body: JSON.stringify({\n                success: false,\n                error: \"Failed to send data to Kinesis stream\",\n                message: error.message\n            }),\n        };\n    }\n}; "
    },
    {
      "path": "/Users/ashutoshsingh/Desktop/personal/infra/infra-cursor/cli/aws-data-pipeline/lambda/streaming-lambda.js",
      "content": "const AWS = require('aws-sdk');\nconst dynamodb = new AWS.DynamoDB.DocumentClient();\n\nexports.handler = async (event) => {\n    console.log('Received Kinesis event:', JSON.stringify(event, null, 2));\n    \n    const tableName = process.env.DYNAMODB_TABLE_NAME;\n    const batchSize = 25; // DynamoDB batch write limit\n    \n    try {\n        // Process Kinesis records\n        const itemsToStore = [];\n        \n        for (const record of event.Records) {\n            try {\n                // Decode the Kinesis data\n                const payload = Buffer.from(record.kinesis.data, 'base64').toString('utf-8');\n                const data = JSON.parse(payload);\n                \n                // Create DynamoDB item\n                const dynamoItem = {\n                    id: data.id || `stream-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`,\n                    data: data,\n                    kinesisMetadata: {\n                        partitionKey: record.kinesis.partitionKey,\n                        sequenceNumber: record.kinesis.sequenceNumber,\n                        approximateArrivalTimestamp: record.kinesis.approximateArrivalTimestamp,\n                        eventSourceARN: record.eventSourceARN\n                    },\n                    processedAt: new Date().toISOString(),\n                    source: 'kinesis-stream',\n                    ttl: Math.floor(Date.now() / 1000) + (30 * 24 * 60 * 60) // 30 days TTL\n                };\n                \n                itemsToStore.push(dynamoItem);\n                \n            } catch (parseError) {\n                console.error('Error parsing Kinesis record:', parseError, record);\n                // Continue processing other records\n            }\n        }\n        \n        // Store items in DynamoDB in batches\n        const batches = [];\n        for (let i = 0; i < itemsToStore.length; i += batchSize) {\n            batches.push(itemsToStore.slice(i, i + batchSize));\n        }\n        \n        const dynamoPromises = batches.map(async (batch, batchIndex) => {\n            const putRequests = batch.map(item => ({\n                PutRequest: {\n                    Item: item\n                }\n            }));\n            \n            const batchWriteParams = {\n                RequestItems: {\n                    [tableName]: putRequests\n                }\n            };\n            \n            try {\n                let attempts = 0;\n                const maxAttempts = 3;\n                let unprocessedItems = batchWriteParams.RequestItems;\n                \n                while (unprocessedItems && Object.keys(unprocessedItems).length > 0 && attempts < maxAttempts) {\n                    attempts++;\n                    \n                    const result = await dynamodb.batchWrite({\n                        RequestItems: unprocessedItems\n                    }).promise();\n                    \n                    console.log(`Batch ${batchIndex}, Attempt ${attempts} result:`, result);\n                    \n                    unprocessedItems = result.UnprocessedItems;\n                    \n                    if (unprocessedItems && Object.keys(unprocessedItems).length > 0) {\n                        // Wait before retry with exponential backoff\n                        const waitTime = Math.pow(2, attempts) * 100;\n                        await new Promise(resolve => setTimeout(resolve, waitTime));\n                        console.log(`Retrying batch ${batchIndex} after ${waitTime}ms...`);\n                    }\n                }\n                \n                if (unprocessedItems && Object.keys(unprocessedItems).length > 0) {\n                    console.error(`Failed to process all items in batch ${batchIndex} after ${maxAttempts} attempts:`, unprocessedItems);\n                    throw new Error(`Failed to process all items in batch ${batchIndex}`);\n                }\n                \n                return { batchIndex, itemsProcessed: batch.length };\n                \n            } catch (dynamoError) {\n                console.error(`Error writing batch ${batchIndex} to DynamoDB:`, dynamoError);\n                throw dynamoError;\n            }\n        });\n        \n        const results = await Promise.all(dynamoPromises);\n        \n        const totalItemsProcessed = results.reduce((sum, result) => sum + result.itemsProcessed, 0);\n        \n        console.log(`Successfully processed ${totalItemsProcessed} items and stored ${batches.length} batches in DynamoDB`);\n        \n        return {\n            statusCode: 200,\n            body: JSON.stringify({\n                recordsProcessed: event.Records.length,\n                itemsStored: totalItemsProcessed,\n                batchesProcessed: batches.length\n            })\n        };\n        \n    } catch (error) {\n        console.error('Error in streaming lambda:', error);\n        throw error; // Let Lambda retry failed records\n    }\n}; "
    },
    {
      "path": "/Users/ashutoshsingh/Desktop/personal/infra/infra-cursor/cli/aws-data-pipeline/package.json",
      "content": "{\n  \"name\": \"aws-data-pipeline\",\n  \"version\": \"1.0.0\",\n  \"description\": \"AWS Data Processing Pipeline with API Gateway, Kinesis, Lambda, SQS, and DynamoDB\",\n  \"main\": \"index.ts\",\n  \"scripts\": {\n    \"build\": \"tsc\",\n    \"deploy\": \"pulumi up\",\n    \"destroy\": \"pulumi destroy\"\n  },\n  \"dependencies\": {\n    \"@pulumi/pulumi\": \"^3.96.0\",\n    \"@pulumi/aws\": \"^6.15.0\",\n    \"@pulumi/awsx\": \"^2.4.0\"\n  },\n  \"devDependencies\": {\n    \"@types/node\": \"^20.0.0\",\n    \"typescript\": \"^5.0.0\"\n  }\n} "
    },
    {
      "path": "/Users/ashutoshsingh/Desktop/personal/infra/infra-cursor/cli/aws-data-pipeline/tsconfig.json",
      "content": "{\n  \"compilerOptions\": {\n    \"target\": \"ES2020\",\n    \"module\": \"commonjs\",\n    \"moduleResolution\": \"node\",\n    \"declaration\": true,\n    \"outDir\": \"./bin\",\n    \"strict\": true,\n    \"noUnusedLocals\": true,\n    \"noUnusedParameters\": true,\n    \"noImplicitReturns\": true,\n    \"noFallthroughCasesInSwitch\": true,\n    \"forceConsistentCasingInFileNames\": true,\n    \"esModuleInterop\": true\n  },\n  \"files\": [\n    \"index.ts\"\n  ]\n} "
    }
  ]
}